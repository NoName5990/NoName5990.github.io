<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Domain Discrepancy Aware Active Learning for Cross-domain
    LiDAR Point Cloud Segmentation</title>
  <link rel="icon" type="image/x-icon" href="static/images/cqupt.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Domain Discrepancy Aware Active Learning for Cross-domain
              LiDAR Point Cloud Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=PUseiVAAAAAJ" target="_blank">Zongyi
                  Xu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://github.com/NoName5990/" target="_blank">Jixiao Liu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=TFp72vEAAAAJ" target="_blank">Shanshan
                  Zhao</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://openreview.net/profile?id=~Zhongpeng_Lang1" target="_blank">Zhongpeng
                  Lang</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=XR6C9BoAAAAJ&hl" target="_blank">Qianni
                  Zhang</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=M17E3HEAAAAJ&hl" target="_blank">Weisheng
                  Li</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=VZVTOOIAAAAJ" target="_blank">Xinbo
                  Gao</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"> <sup>1</sup> <a href="https://www.cqupt.edu.cn/" target="_blank">
                  Chongqing University of Posts and Telecommunications</a>,
                <!-- <br>Conferance name and year -->
              </span>
              <br>
              <span class="author-block"> <sup>2</sup> Alibaba,
                <!-- <a href="https://www.cqupt.edu.cn/" target="_blank">
                  Chongqing University of Posts and Telecommunications</a>  -->
                <!-- <br>Conferance name and year -->
              </span>
              <span class="author-block"> <sup>3</sup> <a href="https://www.qmul.ac.uk/" target="_blank">
                  Queen Mary University of London</a>
                <!-- <br>Conferance name and year -->
              </span>
              <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/main.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="static/pdfs/suppl.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/NoName5990/DDAL" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Paper framework -->
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <img src="static/images/frame.jpg">
        <p style="font-size: 18px;">Overview of the proposed domain discrepancy-aware active learning framework for
          cross-domain LiDAR point
          cloud segmentation. ① Source Prototype Construction. A segmentation model extracts features from the source
          point clouds then using them to construct source prototypes as a semantic representation of source domain. ②
          Source-Prototype Guided Data Selection (SPGDS). A target-to-prototype distance and a source-prototype
          similarity map are calculated, generating a discrepancy score. This discrepancy score is combined with an
          uncertainty score from the prediction to compute a final score, guiding the selection of target candidate
          points for annotation. ③ Balanced Source-Target Mixing Strategy (BSTM). We randomly select an equal number of
          points from the source point cloud to match the annotated target data, then blend them to generate
          intermediate data samples for fine-tuning the segmentation model.</p>
      </div>
    </div>
  </section>

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The generalization of deep learning-based point cloud semantic segmentation models to target domains is
              crucial for its practical use. Although many efforts have been made to explore unsupervised domain
              adaptation (UDA) to transfer knowledge learned from source domains to the target, a significant
              performance disparity remains between the current UDA model and the one trained on a fully annotated
              target domain. Recent active domain adaptation (ADA) approaches seek to enhance the model However, current
              ADA methods for the point cloud segmentation task depend on metrics derived from a model trained on the
              source domain for sample selection, often overlooking the discrepancy between the source and target
              domain. Thus, in this paper, we propose a Domain Discrepancy-aware Active Learning (DDAL) approach for the
              cross-domain LiDAR point cloud semantic segmentation, achieving high segmentation accuracy with minimum
              annotation costs. Firstly, a source-prototype-guided data selection method is proposed to identify and
              annotate those target points which deviate from the data distribution of the source domain. To further
              learn domain-invariant representation, we propose a balanced source-target mixing strategy to mix the
              annotated data with source samples of the same scale. The annotated target data and the randomly selected
              source data construct an intermediate dataset for knowledge transferring, which gradually narrows the
              domain gap between source and target datasets. Extensive experiments on synthetic-to-real and real-to-real
              benchmarks verify that our approach outperforms the state-of-the-arts using only 0.1% active learning
              budget, improving by 7.8%, 8.2%, 4.4%, 5.1%, respectively.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <h2 class="title is-3">Experiments</h2>
        <p>We evaluate our method on several LiDAR semantic segmentation benchmarks, including simulation-to-real and
          real-to-real scenarios.</p>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/s2k.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Table 1: Per-class results on task of SynLiDAR-to-SemanticKITTI.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/s2p.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Table 2: Per-class results on task of SynLiDAR-to-SemanticPOSS.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/s2n.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Table 3: Per-class results on task of SemanticKITTI-to-nuScenes.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/n2s.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Table 4: Per-class results on task of nuScenes-to-SemanticKITTI.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/query.jpg" alt="MY ALT TEXT"/>
            <h2 class="subtitle has-text-centered">
              Table 5: Comparison of different active query strategies for domain adaptation with a mixing rate of 1x.
              Our query strategy consistently achieves superior performance on simulation-to-real and real-to-real
              scenarios.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/ablation.jpg" alt="MY ALT TEXT" width="60%" />
            <h2 class="subtitle has-text-centered">
              Table 6: The ablation study of each component on SynLiDAR-to-SemanticKITTI.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->

  <section class="selection hero">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-12">
          <div class="content has-text-justified" style="align-items: center;">
            <!-- 新增Flex容器包裹图片 -->
            <div class="columns">
              <!-- 左侧图片和文字 -->
              <div class="column">
                <img src="static/images/budget.jpg" width="100%">
                <p class="has-text-centered">Comparisons with different active learning budgets on the task of
                  SynLiDAR-to-SemanticKITTI using our proposed SPGDS query strategy.</p>
              </div>
              <!-- 右侧图片和文字 -->
              <div class="column">
                <img src="static/images/rate.jpg" width="87%">
                <p class="has-text-centered">Comparisons with different mixing rates on SynLiDAR-to-SemanticKITTI using
                  different active learning query strategies.
                </p>
              </div>
            </div>
            <img src="static/images/iteration.jpg">
            <p class="has-text-centered">Visualization of labelled points selected by SPGDS in each iteration on
              SynLiDAR-to-SemanticKITTI.</p>
            <br>
            <img src="static/images/balance.jpg" width=2040 height=1171>
            <p class="has-text-centered">Comparison of category frequencies in SemanticKITTI and the selected point
              subset of our method after five rounds of active learning.</p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Image carousel -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <h2 class="title is-3">Visualizations</h2>
        <p>We also provide the visualization of segmentation results by our method on several LiDAR semantic
          segmentation benchmarks, including simulation-to-real and
          real-to-real scenarios.</p>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/v_s2k.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Visualization of segmentation results for the task SynLiDAR-to-SemanticKITTI.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/v_s2p.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Visualization of segmentation results for the task SynLiDAR-to-SemanticPOSS.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/v_s2n.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Visualization of segmentation results for the task SemanticKITTI-to-nuScenes.
            </h2>
          </div>
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/v_n2s.jpg" alt="MY ALT TEXT" />
            <h2 class="subtitle has-text-centered">
              Visualization of segmentation results for the task nuScenes-to-SemanticKITTI.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End image carousel -->






  <!-- Teaser video-->
  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/videos/banner_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat
          pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.
        </h2>
      </div>
    </div>
  </section> -->
  <!-- End teaser video -->


  <!-- Youtube video -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Video Presentation</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="publication-video">
              <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media"
                allowfullscreen></iframe>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End youtube video -->


  <!-- Video carousel -->
  <!-- <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <h2 class="title is-3">Another Carousel</h2>
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-video1">
            <video poster="" id="video1" autoplay controls muted loop height="100%">
              Your video file here
              <source src="static/videos/carousel1.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video2">
            <video poster="" id="video2" autoplay controls muted loop height="100%">
              Your video file here
              <source src="static/videos/carousel2.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-video3">
            <video poster="" id="video3" autoplay controls muted loop height="100%">\
               Your video file here
              <source src="static/videos/carousel3.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->
  <!-- End video carousel -->


  <!-- Paper poster -->
  <!-- <section class="hero is-small is-light">
    <div class="hero-body">
      <div class="container">
        <h2 class="title">Poster</h2>

        <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
        </iframe>

      </div>
    </div>
  </section> -->
  <!--End paper poster -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>